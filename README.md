# Wave Breaking Classification with Knowledge Distillation

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯çš„æ³¢æµªç ´ç¢åˆ†ç±»é¡¹ç›®ï¼Œé‡‡ç”¨DINOv2ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œè‡ªå®šä¹‰ViT+LSTMä½œä¸ºå­¦ç”Ÿæ¨¡å‹ã€‚

## ğŸŒŠ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä¸“é—¨é’ˆå¯¹æµ·æ´‹æ³¢æµªç ´ç¢ç°è±¡çš„åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯å°†å¤§å‹DINOv2æ¨¡å‹çš„çŸ¥è¯†ä¼ é€’ç»™è½»é‡çº§çš„CRNNç½‘ç»œã€‚

### ç‰¹æ€§

- **æ•™å¸ˆæ¨¡å‹**: DINOv2 ViT-Base (86Må‚æ•°)
- **å­¦ç”Ÿæ¨¡å‹**: è‡ªå®šä¹‰ViT + LSTM CRNNç½‘ç»œ (~20Må‚æ•°)
- **åˆ†ç±»ç±»åˆ«**: 3ç±»æ³¢æµªç ´ç¢ç±»å‹
  - Plunging (å·ç ´)
  - Spilling (æº¢ç ´) 
  - Surging (æ¶Œç ´)
- **æŠ€æœ¯æ ˆ**: PyTorch Lightning + Hydra + WandB
- **çŸ¥è¯†è’¸é¦**: ç»“åˆlogitsè’¸é¦å’Œç‰¹å¾è’¸é¦

## ğŸ“ é¡¹ç›®ç»“æ„

```
wave_distillation/
â”œâ”€â”€ configs/                 # Hydraé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ config.yaml         # ä¸»é…ç½®
â”‚   â”œâ”€â”€ data/               # æ•°æ®é…ç½®
â”‚   â”œâ”€â”€ model/              # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ trainer/            # è®­ç»ƒå™¨é…ç½®
â”‚   â””â”€â”€ logger/             # æ—¥å¿—é…ç½®
â”œâ”€â”€ src/                    # æºä»£ç 
â”‚   â”œâ”€â”€ data/               # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ models/             # æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ utils/              # å·¥å…·å‡½æ•°
â”œâ”€â”€ main.py                 # ä¸»è¿è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt        # ä¾èµ–æ–‡ä»¶
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd wave_distillation

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n wave_distill python=3.9
conda activate wave_distill

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

å°†æ‚¨çš„æ•°æ®é›†æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
your_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ plunging/
â”‚   â”‚   â”œâ”€â”€ img001.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ spilling/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ surging/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ plunging/
â”‚   â”œâ”€â”€ spilling/
â”‚   â””â”€â”€ surging/
â””â”€â”€ test/ (å¯é€‰)
    â”œâ”€â”€ plunging/
    â”œâ”€â”€ spilling/
    â””â”€â”€ surging/
```

### 3. é…ç½®ä¿®æ”¹

ä¿®æ”¹ `configs/data/wave_breaking_enhanced.yaml` ä¸­çš„æ•°æ®è·¯å¾„ï¼š

```yaml
data_dir: "/path/to/your/wave_dataset"  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ
python main.py

# æŒ‡å®šæ•°æ®è·¯å¾„
python main.py data.data_dir="/path/to/your/dataset"

# è°ƒæ•´è¶…å‚æ•°
python main.py model.learning_rate=1e-4 data.batch_size=16

# ä½¿ç”¨ä¸åŒé…ç½®
python main.py data=wave_breaking model.temperature=4.0
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ•°æ®é…ç½®

- **wave_breaking.yaml**: åŸºç¡€é…ç½®ï¼Œä½¿ç”¨æ ‡å‡†æ•°æ®å¢å¼º
- **wave_breaking_enhanced.yaml**: å¢å¼ºé…ç½®ï¼ŒåŒ…å«æ³¢æµªç‰¹å®šå¢å¼º

### æ¨¡å‹é…ç½®

- **teacher**: DINOv2é…ç½®
- **student**: ViT+LSTMé…ç½®
- **loss_weights**: æŸå¤±æƒé‡é…ç½®

### è®­ç»ƒé…ç½®

- **max_epochs**: æœ€å¤§è®­ç»ƒè½®æ•°
- **callbacks**: å›è°ƒå‡½æ•°ï¼ˆæ£€æŸ¥ç‚¹ã€æ—©åœç­‰ï¼‰
- **precision**: æ··åˆç²¾åº¦è®­ç»ƒ

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

é¡¹ç›®é›†æˆäº†WandBè¿›è¡Œå®éªŒè¿½è¸ªï¼š

1. æ³¨å†ŒWandBè´¦å·ï¼šhttps://wandb.ai
2. ç™»å½•ï¼š`wandb login`
3. è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è®°å½•ï¼š
   - æŸå¤±æ›²çº¿
   - å‡†ç¡®ç‡æŒ‡æ ‡
   - æ··æ·†çŸ©é˜µ
   - æ¨¡å‹å‚æ•°ç»Ÿè®¡

## ğŸ”§ è‡ªå®šä¹‰ä¿®æ”¹

### æ·»åŠ æ–°çš„æ•°æ®æ ¼å¼

1. åœ¨ `src/data/custom_dataset.py` ä¸­æ·»åŠ æ–°çš„åŠ è½½æ–¹æ³•
2. åˆ›å»ºå¯¹åº”çš„é…ç½®æ–‡ä»¶
3. æ›´æ–°æ•°æ®æ¨¡å—

### ä¿®æ”¹æ¨¡å‹æ¶æ„

1. ç¼–è¾‘ `src/models/student_model.py` ä¸­çš„ç½‘ç»œç»“æ„
2. è°ƒæ•´ `configs/model/distillation.yaml` ä¸­çš„å‚æ•°
3. é‡æ–°è®­ç»ƒ

### æ·»åŠ æ–°çš„æŸå¤±å‡½æ•°

1. åœ¨ `src/models/distillation_model.py` ä¸­å®šä¹‰æ–°æŸå¤±
2. åœ¨è®­ç»ƒæ­¥éª¤ä¸­é›†æˆ
3. æ·»åŠ å¯¹åº”çš„é…ç½®é€‰é¡¹

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### æ•°æ®åŠ è½½ä¼˜åŒ–

```bash
# å¢åŠ æ•°æ®åŠ è½½çº¿ç¨‹
python main.py data.num_workers=8

# å¯ç”¨åŠ æƒé‡‡æ ·
python main.py data.use_weighted_sampling=true
```

### è®­ç»ƒä¼˜åŒ–

```bash
# ä½¿ç”¨æ··åˆç²¾åº¦
python main.py trainer.precision="16-mixed"

# æ¢¯åº¦ç´¯ç§¯
python main.py trainer.accumulate_grad_batches=2

# è°ƒæ•´æ‰¹æ¬¡å¤§å°
python main.py data.batch_size=64
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   ```bash
   python main.py data.batch_size=16 trainer.precision="16-mixed"
   ```

2. **æ•°æ®åŠ è½½é”™è¯¯**
   - æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤å›¾åƒæ ¼å¼æ”¯æŒ
   - éªŒè¯æ–‡ä»¶å¤¹ç»“æ„

3. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆDINOv2éœ€è¦ä¸‹è½½ï¼‰
   - ç¡®è®¤transformersç‰ˆæœ¬å…¼å®¹æ€§

### è°ƒè¯•æ¨¡å¼

```bash
# å¿«é€Ÿè°ƒè¯•ï¼ˆå°‘é‡æ•°æ®ï¼‰
python main.py trainer.fast_dev_run=true

# è¯¦ç»†æ—¥å¿—
python main.py hydra.verbose=true
```

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ç›¸å…³è®ºæ–‡ï¼š

```bibtex
@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, TimothÃ©e and Moutakanni, Theo and Vo, Huy V and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“§ è”ç³»

å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»ï¼š[your-email@example.com]