model:
  _target_: src.models.distillation_model.DistillationLightningModule
  num_classes: 3
  teacher:
    model_name: dinov2-base
    freeze: true
  student:
    vit:
      image_size: 224
      patch_size: 16
      num_classes: 3
      dim: 384
      depth: 6
      heads: 8
      mlp_dim: 768
      dropout: 0.1
      emb_dropout: 0.1
    lstm:
      hidden_size: 384
      num_layers: 3
      dropout: 0.2
      bidirectional: true
  loss_weights:
    classification: 1.0
    distillation: 3.0
  learning_rate: 0.0002
  weight_decay: 0.0001
  temperature: 3.0
data:
  _target_: src.data.wave_datamodule.WaveDataModule
  data_dir: ./data
  batch_size: 32
  num_workers: 4
  image_size: 224
  use_wave_augmentation: true
  use_weighted_sampling: true
  nested_folders: true
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  max_epochs: 100
  accelerator: auto
  devices: auto
  strategy: auto
  callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: val/accuracy
    mode: max
    save_top_k: 3
    filename: best-{epoch:02d}-{val/accuracy:.4f}
    save_last: true
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/accuracy
    mode: max
    patience: 20
    min_delta: 0.001
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  - _target_: lightning.pytorch.callbacks.RichProgressBar
  precision: 16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 50
logger:
  _target_: lightning.pytorch.loggers.WandbLogger
  project: wave-breaking-distillation
  name: null
  save_dir: ./logs
  log_model: true
  tags:
  - knowledge_distillation
  - wave_breaking
  - dinov2
  - vit_lstm
seed: 42
experiment_name: wave_breaking_distillation
